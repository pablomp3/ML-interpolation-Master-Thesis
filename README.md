## ML-interpolation-Master-Thesis
# Variational Autoencoders for Polyphonic Music Interpolation
This is the title of my Master Thesis submitted to National Tsing Hua University (Taiwan, July 2020)

### **Abstract**

This thesis aims to use Machine Learning techniques to solve the novel problem of music interpolation composition. Two models based on Variational Autoencoders (VAEs) are proposed to generate a suitable polyphonic harmonic bridge between two given songs, smoothly changing the pitches and dynamics of the interpolation. The interpolations generated by the first model surpass a Random baseline data and a bidirectional LSTM approaches and its performance is comparable to the current state-of-the-art. The novel architecture of the second model outperforms the state-of-the-art interpolation approaches in terms of reconstruction loss by using an additional neural network for direct estimation of the interpolation encoded vector. Furthermore, the _Hsinchu Interpolation MIDI Dataset_ was created, making both models proposed in this thesis more efficient than previous approaches in the literature in terms of computational and time requirements during training. Finally, a quantitative user study was done in order to ensure the validity of the results.

### **What do we mean by music interpolation?**

“Interpolation is a type of **estimation**, a method of constructing **new data** points within the range of a discrete set of known data points” (Fleetwood, 1991)

In traditional Machine Learning, the generation of music is conditioned on the past events. But what if we could condition the music generation on both past and future events? We would input a begin track and an end track to our model, obtaining the middle (or interpolation) track as output, whose pitches and dynamics match both given tracks.

![](https://github.com/pablomp3/ML-interpolation-Master-Thesis/blob/master/images/interpolation_definition.jpg)

### **What is polyphonic music and how to model it?**

In **monophonic** music, every timestep or time unit contains one single note. On the other hand, in **polyphonic** music, every timestep contains several notes, forming chords that make the composition richer. We use MIDI (Musical Instrument Digital Interface) to represent the music in a symbolic way, instead of using the raw waveform (which is computationally expensive to manipulate). Each timestep of a song is represented as a vector of 64 binary elements, where each binary element represents one piano key (or one note or pitch), 1 meaning _note on_ and 0 meaning _note off_.

![](https://github.com/pablomp3/ML-interpolation-Master-Thesis/blob/master/images/mono_vs_polyphonic.jpg)

![](https://github.com/pablomp3/ML-interpolation-Master-Thesis/blob/master/images/polyphonic_modelling.jpg)
